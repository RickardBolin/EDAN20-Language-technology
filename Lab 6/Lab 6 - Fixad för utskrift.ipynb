{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #6: Dependency parsing\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by the CoNLL 2018 shared task of the conference on computational natural language learning on dependency parsing, http://universaldependencies.org/conll18/. It is a follower of <a href=\"http://ilk.uvt.nl/conll/\">CONLL-X</a>, which was the first large-scale evaluation of dependency parsers.\n",
    "            \n",
    "In this session, you will implement a dependency parser for Swedish and, optionally, another language that you will choose.\n",
    "\n",
    "The objectives of this assignment are to:\n",
    "* Know what a dependency graph is\n",
    "* Understand the principles of a transition-based parser\n",
    "* Extend the parser with a guiding predicate that parses an annotated dependency graph\n",
    "* Extract features to learn parsing actions from an annotated corpus\n",
    "* Write a short report on your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization and location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can work alone or collaborate with another student.\n",
    "Each group will have to:\n",
    "* Write a program that parses a sentence when the dependency graph is known\n",
    "* Extract features from the parsing actions.\n",
    "* Train a classifier\n",
    "* Apply it on a test corpus\n",
    "* Evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names of the CoNLL-U corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names_u = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to read the CoNLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file, encoding='utf-8').read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    root_values = ['0', 'root', 'root', 'root', 'root', 'root', '0', 'root', 'root', 'root']\n",
    "    start = [dict(zip(column_names, root_values))]\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split('\\t'))) for row in rows if row[0] != '#']\n",
    "        sentence = start + sentence\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Swedish _Talbanken_ corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv_train)\n",
    "formatted_corpus_train = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed sentence: _Individuell beskattning av arbetsinkomster_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing indices that are not integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the processing of some corpora, we remove the indices which are not integers. We do this because `ID` is not necessarily a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_indicies(formatted_corpus):\n",
    "    formatted_corpus_clean = []\n",
    "    for sentence in formatted_corpus:\n",
    "        formatted_corpus_clean.append([word for word in sentence if word['ID'].isdigit()])\n",
    "    return formatted_corpus_clean          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence with a projective dependency graph, there is an action sequence that enables the transition parser\n",
    "to generate this graph. Gold standard parsing corresponds to the sequence of parsing actions, left-arc (<tt>la</tt>), right-arc (<tt>ra</tt>), shift (<tt>sh</tt>), and reduce (<tt>re</tt>) that produces the manually-obtained, gold standard, graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are implementations of the parsing transitions. Read them and be sure you understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift(stack, queue, graph):\n",
    "    \"\"\"\n",
    "    Shift the first word in the queue onto the stack\n",
    "    :param stack:\n",
    "    :param queue:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    stack = [queue[0]] + stack\n",
    "    queue = queue[1:]\n",
    "    return stack, queue, graph\n",
    "\n",
    "\n",
    "def reduce(stack, queue, graph):\n",
    "    \"\"\"\n",
    "    Remove the first item from the stack\n",
    "    :param stack:\n",
    "    :param queue:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return stack[1:], queue, graph\n",
    "\n",
    "\n",
    "def right_arc(stack, queue, graph, deprel=False):\n",
    "    \"\"\"\n",
    "    Creates an arc from the top of the stack to the first in the queue\n",
    "    and shifts\n",
    "    The deprel argument is either read from the manually-annotated corpus\n",
    "    (deprel=False) or assigned by the parser. In this case, the deprel\n",
    "    argument has a value\n",
    "    :param stack:\n",
    "    :param queue:\n",
    "    :param graph:\n",
    "    :param deprel: either read from the manually-annotated corpus (value false)\n",
    "    or assigned by the parser\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    graph['heads'][queue[0]['ID']] = stack[0]['ID']\n",
    "    if deprel:\n",
    "        graph['deprels'][queue[0]['ID']] = deprel\n",
    "    else:\n",
    "        graph['deprels'][queue[0]['ID']] = queue[0]['DEPREL']\n",
    "    # If we create an arc from the 'root', we introduce a statement to pop it to avoid multiple roots\n",
    "    if stack[0]['ID'] == '0':\n",
    "        stack = stack[1:]\n",
    "    return shift(stack, queue, graph)\n",
    "\n",
    "\n",
    "def left_arc(stack, queue, graph, deprel=False):\n",
    "    \"\"\"\n",
    "    Creates an arc from the first in the queue to the top of the stack\n",
    "    and reduces it.\n",
    "    The deprel argument is either read from the manually-annotated corpus\n",
    "    (deprel=False) or assigned by the parser. In this case, the deprel\n",
    "    argument has a value\n",
    "    :param stack:\n",
    "    :param queue:\n",
    "    :param graph:\n",
    "    :param deprel: either read from the manually-annotated corpus (value false)\n",
    "    or assigned by the parser\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    graph['heads'][stack[0]['ID']] = queue[0]['ID']\n",
    "    if deprel:\n",
    "        graph['deprels'][stack[0]['ID']] = deprel\n",
    "    else:\n",
    "        graph['deprels'][stack[0]['ID']] = stack[0]['DEPREL']        \n",
    "    return reduce(stack, queue, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrains on the transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a few constraints before we carry out the transitions. Given a manually-annotated dependency graph, look at the conditions (`can_...()` functions) on the stack and the current input list -- the queue -- to execute left-arc, right-arc, shift, or reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def can_reduce(stack, graph):\n",
    "    \"\"\"\n",
    "    Checks that the top of the stack has a head\n",
    "    :param stack:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not stack:\n",
    "        return False\n",
    "    if stack[0]['ID'] in graph['heads']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def can_leftarc(stack, graph):\n",
    "    \"\"\"\n",
    "    Checks that the top of the has no head\n",
    "    :param stack:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not stack:\n",
    "        return False\n",
    "    if stack[0]['ID'] in graph['heads']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def can_rightarc(stack):\n",
    "    \"\"\"\n",
    "    Simply checks there is a stack\n",
    "    :param stack:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not stack:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the transitions from a manually-parsed sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an annotated corpus, we can derive the action sequences producing the manually-parsed sentences (provided that they are projective). We use an oracle for this as explained during the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oracle(stack, queue, graph):\n",
    "    \"\"\"\n",
    "    Gold standard parsing\n",
    "    Produces a sequence of transitions from a manually-annotated corpus:\n",
    "    sh, re, ra.deprel, la.deprel\n",
    "    :param stack: The stack\n",
    "    :param queue: The input list\n",
    "    :param graph: The set of relations already parsed\n",
    "    :return: the transition and the grammatical function (deprel) in the\n",
    "    form of transition.deprel\n",
    "    \"\"\"\n",
    "    # Right arc\n",
    "    if stack and stack[0]['ID'] == queue[0]['HEAD']:\n",
    "        # print('ra', queue[0]['DEPREL'], stack[0]['UPOS'], queue[0]['UPOS'])\n",
    "        deprel = '.' + queue[0]['DEPREL']\n",
    "        stack, queue, graph = right_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'ra' + deprel\n",
    "    # Left arc\n",
    "    if stack and queue[0]['ID'] == stack[0]['HEAD']:\n",
    "        # print('la', stack[0]['DEPREL'], stack[0]['UPOS'], queue[0]['UPOS'])\n",
    "        deprel = '.' + stack[0]['DEPREL']\n",
    "        stack, queue, graph = left_arc(stack, queue, graph)\n",
    "        return stack, queue, graph, 'la' + deprel\n",
    "    # Reduce\n",
    "    if stack and can_reduce(stack, graph):\n",
    "        for word in stack:\n",
    "            if (word['ID'] == queue[0]['HEAD'] or\n",
    "                    word['HEAD'] == queue[0]['ID']):\n",
    "                # print('re', stack[0]['UPOS'], queue[0]['UPOS'])\n",
    "                stack, queue, graph = reduce(stack, queue, graph)\n",
    "                return stack, queue, graph, 're'\n",
    "    # Shift\n",
    "    # print('sh', [], queue[0]['UPOS'])\n",
    "    stack, queue, graph = shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with nonprojective graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle parsing produces a sequence of transitions if the graph is projective and well-formed. If not, we will have headless words in the stack. Parsing normally terminates when the queue is empty. We also empty the stack to be sure that all the words have a head. We attach headless words to the root word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exists_root(graph):\n",
    "    for (x, y)  in graph['heads'].items():\n",
    "        if y == '0' and x != '0':\n",
    "            return x\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_stack(stack, graph):\n",
    "    \"\"\"\n",
    "    Pops the items in the stack. If they have no head, they are assigned\n",
    "    a ROOT head\n",
    "    :param stack:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idx_root = exists_root(graph)\n",
    "    # There is already a root\n",
    "    if idx_root:\n",
    "        for word in stack:\n",
    "            if word['ID'] not in graph['heads']:\n",
    "                graph['heads'][word['ID']] = idx_root\n",
    "                graph['deprels'][word['ID']] = 'dep'\n",
    "    else:\n",
    "        # There is no root. We assign the root to the first headless word.\n",
    "        for word in stack:\n",
    "            if word['ID'] not in graph['heads']:\n",
    "                if idx_root:\n",
    "                    graph['heads'][word['ID']] = idx_root\n",
    "                    graph['deprels'][word['ID']] = 'dep'\n",
    "                else:\n",
    "                    graph['heads'][word['ID']] = '0'\n",
    "                    graph['deprels'][word['ID']] = 'root'\n",
    "                    idx_root = word['ID']\n",
    "    stack = []\n",
    "    return stack, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if two graphs are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `equal_graphs()` utility checks if the graph obtained from a sequence of transitions is equal to the annotated graph. It is normally the case, except with nonprojective graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equal_graphs(sentence, graph, verbose=False):\n",
    "    \"\"\"\n",
    "    Checks that the graph corresponds to the gold standard annotation of a sentence\n",
    "    :param sentence:\n",
    "    :param graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    equal = True\n",
    "    for word in sentence:\n",
    "        if word['ID'] in graph['heads'] and word['HEAD'] == graph['heads'][word['ID']]:\n",
    "            pass\n",
    "        else:\n",
    "            equal = False\n",
    "            if verbose:\n",
    "                print(word, flush=True)\n",
    "    return equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing an annotated corpus with an oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run the code below. With it, you will produce a sequence of transitions for each sentence. If the graph is projective, applying the sequence to the sentence will recreate the gold-standard annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment:\n",
    "1. Understand from the slides used during the lecture how the oracle carries out a gold standard parsing. \n",
    "2. The parser can only deal with projective sentences. In the case of a nonprojective one, the parsed graph and the manually-annotated sentence are not equal. Examine one nonprojective sentence (just set `verbose`to `True` in the code below) and explain why it is not projective. Take a short one (the shortest). You will **describe** this in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_config(sentence):\n",
    "    stack = []\n",
    "    queue = list(sentence)\n",
    "    graph = {}\n",
    "    graph['heads'] = {}\n",
    "    graph['heads']['0'] = '0'\n",
    "    graph['deprels'] = {}\n",
    "    graph['deprels']['0'] = 'ROOT'\n",
    "    return stack, queue, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "projectivization = False\n",
    "\n",
    "transition_corpus = []\n",
    "graph_corpus = []\n",
    "\n",
    "for sent_cnt, sentence in enumerate(formatted_corpus_train_clean):\n",
    "    #print(sentence)\n",
    "    stack, queue, graph = init_config(sentence)\n",
    "    transition_sent = []\n",
    "    while queue:\n",
    "        stack, queue, graph, trans = oracle(stack, queue, graph)\n",
    "        transition_sent.append(trans)\n",
    "    stack, graph = empty_stack(stack, graph)\n",
    "    transition_corpus.append(transition_sent)\n",
    "    graph_corpus.append(graph)\n",
    "\n",
    "    if verbose:\n",
    "        if not equal_graphs(sentence, graph):\n",
    "            print('Annotation and gold-standard parsing not equal')\n",
    "            print('Sentence:', sentence)\n",
    "            print('Gold-standard graph', graph)\n",
    "    # Poorman's projectivization to have well-formed graphs.\n",
    "    # We just just assign the same heads as what gold standard parsing did\n",
    "    # This guarantee a projective sentence\n",
    "    if projectivization:\n",
    "        for word in sentence:\n",
    "            word['HEAD'] = graph['heads'][word['ID']]\n",
    "print('\\nProcessed ' + str(sent_cnt) + ' sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a classifier to predict an action from a current parsing context. To be able to predict the next action from a given parsing state, gold standard parsing must also extract feature vectors at each step of the parsing procedure. The simplest parsing context corresponds to words' part of speech on the top of the stack and head of the input list (the queue).\n",
    "    \n",
    "Once the data collected, the training procedure will produce a 4-class classifier that you will embed in\n",
    "Nivre's parser to choose the next action. During parsing, Nivre's parser will call the classifier to choose\n",
    "the next action in the set {<tt>la</tt>, <tt>ra</tt>, <tt>sh</tt>, <tt>re</tt>} using the current context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use two feature sets to build your models:\n",
    "* The top of the stack and the first word of the input list (word forms and parts of speech);\n",
    "* The two first words and POS on the top of the stack and the two first words and POS of the input list;\n",
    "* You will also add constraints to actions. You will encode these constraints as Boolean features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the grammatical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the actions in the set {<tt>la</tt>, <tt>ra</tt>, <tt>sh</tt>, <tt>re</tt>} produces an unlabelled\n",
    "graph. It is easy to extend the parser so that it can label the graph with grammatical functions. In this\n",
    "case, we must complement the actions <tt>la</tt>\n",
    "and <tt>ra</tt> with their function using this notation for example:<tt>la.mod</tt>, <tt>la.case</tt>, <tt>ra.nmod</tt>, etc. where the prefix is the action and the suffix is the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal is to parse the Swedish corpus and produce a labelled dependency graph. \n",
    "\n",
    "You will consider two feature sets and you will train the corresponding logistic regression models using scikit-learn:\n",
    "1. The first set will use the word and the part of speech extracted from the first element in the stack and the first in the queue,\n",
    "2. the second one will use two elements from the stack and two from the input list.\n",
    "\n",
    "These sets will include two additional Boolean parameters, \"can do left arc\" and \"can do reduce\", which will model constraints on the parser's actions. In total, the feature sets will then have six, respectively ten parameters.\n",
    "\n",
    "This means that the purpose of this assignment is to generate two scikit-learn models for the labelled graphs. We use the depth parameter for this: The depth of the stack and the queue, either 1 or 2. Start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the `queue_stack()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queue_stack(queue_or_stack, graph, depth, pos=True, lex=True):\n",
    "    features = []\n",
    "    features_pos = ['nil'] * depth\n",
    "    features_lex = ['nil'] * depth\n",
    "    features_deprel = ['nil'] * depth\n",
    "    if queue_or_stack:\n",
    "        for i, word in list(enumerate(queue_or_stack))[:depth]:\n",
    "            features_pos[i] = queue_or_stack[i]['UPOS']\n",
    "            features_lex[i] = queue_or_stack[i]['FORM']\n",
    "    if pos:\n",
    "        features += features_pos\n",
    "    if lex:\n",
    "        features += features_lex\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you may want to extend the feature vector with words to the left of the top of the stack with the `right_context()` function. If the top of the stack has index $i$, you will extract the words and their parts of speech at index $i + 1$, $i+2$. This will noticeably improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def right_context(stack, sentence, depth, pos=True, lex=True):\n",
    "    features = []\n",
    "    features_pos = ['nil'] * depth\n",
    "    features_lex = ['nil'] * depth\n",
    "    if stack:\n",
    "        fw_id = int(stack[0]['ID']) + 1\n",
    "        for i, word in list(enumerate(sentence))[fw_id: fw_id + depth]:\n",
    "            features_pos[i - fw_id] = sentence[i]['UPOS']\n",
    "            features_lex[i - fw_id] = sentence[i]['FORM']\n",
    "    if pos:\n",
    "        features += features_pos\n",
    "    if lex:\n",
    "        features += features_lex\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function returns the features in a dictionary format compatible with scikit-learn. You have a code example of feature encoding in this format in the chunking program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(depth, stack, queue, graph, sentence):\n",
    "    \"\"\"\n",
    "    :param stack:\n",
    "    :param queue:\n",
    "    :param graph:\n",
    "    :param feature_names:\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    improved = True\n",
    "    if improved:\n",
    "        x = (queue_stack(stack, graph, depth) +\n",
    "             queue_stack(queue, graph, depth) +\n",
    "             right_context(stack, sentence, depth) +\n",
    "             [can_reduce(stack, graph), can_leftarc(stack, graph)])\n",
    "    else:\n",
    "        x = (queue_stack(stack, graph, depth) +\n",
    "             queue_stack(queue, graph, depth) +\n",
    "             [can_reduce(stack, graph), can_leftarc(stack, graph)])     \n",
    "    feature_names = ['feat' + str(i) for i in range(len(x))]\n",
    "    features = dict(zip(feature_names, x))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a loop to parse the annotated corpus using the oracle and collect the features in a matrix ($\\mathbf{X}$) and the transitions in a vector ($\\mathbf{y}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lines of your features for the 4 parameters ($\\mathbf{x}$) and labelled actions ($y$) should look like the excerpt below, where the columns correspond to stack0_POS, stack1_POS, stack0_word, stack1_word, queue0_POS, queue1_POS, queue0_word, queue1_word, can-re, can-la, and the transition value (`depth = 2`):\n",
    "$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "\\text{nil}& \\text{nil} &\\text{nil} & \\text{nil} & \\text{ROOT} & \\text{ADJ} & \\text{ROOT} & \\text{Individuell} & \\text{False} & \\text{False}\\\\\n",
    "\\text{ROOT} &     \\text{nil} &     \\text{ROOT} &     \\text{nil} &     \\text{ADJ} &     \\text{NOUN} &     \\text{Individuell} &     \\text{beskattning} &     \\text{True} &     \\text{False}\\\\ \n",
    "\\text{ADJ} &     \\text{ROOT} &     \\text{Individuell} &     \\text{ROOT} &     \\text{NOUN} &     \\text{ADP} &     \\text{beskattning} &     \\text{av} &     \\text{False} &     \\text{True}\\\\ \n",
    "\\text{ROOT} &     \\text{nil} &     \\text{ROOT} &     \\text{nil} &     \\text{NOUN} &     \\text{ADP} &     \\text{beskattning} &     \\text{av} &     \\text{True} &     \\text{False}\\\\\n",
    "\\text{NOUN} &     \\text{ROOT} &     \\text{beskattning} &     \\text{ROOT} &     \\text{ADP} &     \\text{NOUN} &     \\text{av} &     \\text{arbetsinkomster} &     \\text{True} &     \\text{False}\\\\\n",
    "\\text{ADP} &     \\text{NOUN} &     \\text{av} &     \\text{beskattning} &     \\text{NOUN} &     \\text{nil} &     \\text{arbetsinkomster} &     \\text{nil} &     \\text{False} &     \\text{True}\\\\  \\text{NOUN} &     \\text{ROOT} &     \\text{beskattning} &     \\text{ROOT} &     \\text{NOUN} &     \\text{nil} &     \\text{arbetsinkomster} &     \\text{nil} &     \\text{True} &  \\text{False}\n",
    "\\end{bmatrix}$\n",
    "; $\\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "\\text{sh}\\\\\n",
    "\\text{sh}\\\\\n",
    "\\text{la.amod}\\\\\n",
    "\\text{ra.root}\\\\\n",
    "\\text{sh}\\\\\n",
    "\\text{la.case}\\\\\n",
    "\\text{ra.nmod}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will store your matrix in a Python dictionary and the classes in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dict = []\n",
    "y_symbols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [00:03, 1187.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def apply_transition(stack, queue, graph, trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    if stack and can_leftarc(stack, graph) and trans[:2] == 'la':\n",
    "        if trans[3:] == 'root' and exists_root(graph):\n",
    "            stack, queue, graph = shift(stack, queue, graph)\n",
    "            return stack, queue, graph, 'sh'\n",
    "        else:\n",
    "            stack, queue, graph = left_arc(stack, queue, graph, trans[3:])\n",
    "            return stack, queue, graph, 'la'\n",
    "    if stack and can_reduce(stack, graph) and trans == 're':\n",
    "        stack, queue, graph = reduce(stack, queue, graph)\n",
    "        return stack, queue, graph, 're'\n",
    "    stack, queue, graph = shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'\n",
    "\n",
    "for (sentence, transition) in tqdm(zip(formatted_corpus_train_clean, transition_corpus)):\n",
    "    stack, queue, graph = init_config(sentence)\n",
    "    i = 0\n",
    "    while queue:\n",
    "        X_dict.append(extract(depth, stack, queue, graph, sentence))\n",
    "        y_symbols.append(transition[i])\n",
    "        stack, queue, graph, trans = apply_transition(stack, queue, graph, transition[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sh', 'sh', 'la.amod', 'ra.root', 'sh', 'la.case', 'ra.nmod']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_symbols[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize your `X_dict` into an `X` matrix using `DictVectorizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import linear_model\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model. With sklearn, you can use `y_symbols` directly. Use `verbose=True` and `n_jobs=8` or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=8, verbose=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression(verbose=True, n_jobs=8)\n",
    "model = classifier.fit(X, y_symbols)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this model to predict the sentences in the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences_test = read_sentences(path_sv_test)\n",
    "formatted_corpus_test = split_rows(sentences_test, column_names_u)\n",
    "formatted_corpus_test_clean = clean_indicies(formatted_corpus_test)\n",
    "formatted_corpus_test_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transition(stack, queue, graph, trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    if stack and can_leftarc(stack, graph) and trans[:2] == 'la':\n",
    "        if trans[3:] == 'root' and exists_root(graph):\n",
    "            stack, queue, graph = shift(stack, queue, graph)\n",
    "            return stack, queue, graph, 'sh'\n",
    "        else:\n",
    "            stack, queue, graph = left_arc(stack, queue, graph, trans[3:])\n",
    "            return stack, queue, graph, 'la'\n",
    "    if stack and can_reduce(stack, graph) and trans == 're':\n",
    "        stack, queue, graph = reduce(stack, queue, graph)\n",
    "        return stack, queue, graph, 're'\n",
    "    stack, queue, graph = shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1219it [15:27,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sent_cnt, sentence in tqdm(enumerate(formatted_corpus_test_clean)):\n",
    "    X_test_dict = []\n",
    "    stack, queue, graph = init_config(sentence)\n",
    "    while queue:\n",
    "        X_test_dict = extract(depth, stack, queue, graph, sentence)\n",
    "        X_test = vec.transform(X_test_dict)\n",
    "        y_test = classifier.predict(X_test)[0]\n",
    "        stack, queue, graph, trans = apply_transition(stack, queue, graph, y_test)\n",
    "    stack, graph = empty_stack(stack, graph)\n",
    "    for word in sentence:\n",
    "        word['HEAD'] = graph['heads'][word['ID']]\n",
    "        word['DEPREL'] = graph['deprels'][word['ID']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
