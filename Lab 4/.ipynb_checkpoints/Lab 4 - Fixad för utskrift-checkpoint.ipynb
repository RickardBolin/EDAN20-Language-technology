{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. In addition, you will try to link a few extracted named entities to real things using wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures\n",
    "* Extract named entities and link them to real things using Wikipedia\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: scikit-learn\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]\n",
    "\n",
    "\"\"\"\n",
    "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
    "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
    "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
    "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
    "    ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline performance corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect all the parts of speech and we count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 30147,\n",
       " 'IN': 22764,\n",
       " 'DT': 18335,\n",
       " 'VBZ': 4648,\n",
       " 'RB': 6607,\n",
       " 'VBN': 4763,\n",
       " 'TO': 5081,\n",
       " 'VB': 6017,\n",
       " 'JJ': 13085,\n",
       " 'NNS': 13619,\n",
       " 'NNP': 19884,\n",
       " ',': 10770,\n",
       " 'CC': 5372,\n",
       " 'POS': 1769,\n",
       " '.': 8827,\n",
       " 'VBP': 2868,\n",
       " 'VBG': 3272,\n",
       " 'PRP$': 1881,\n",
       " 'CD': 8315,\n",
       " '``': 1531,\n",
       " \"''\": 1493,\n",
       " 'VBD': 6745,\n",
       " 'EX': 206,\n",
       " 'MD': 2167,\n",
       " '#': 36,\n",
       " '(': 274,\n",
       " '$': 1750,\n",
       " ')': 281,\n",
       " 'NNPS': 420,\n",
       " 'PRP': 3820,\n",
       " 'JJS': 374,\n",
       " 'WP': 529,\n",
       " 'RBR': 321,\n",
       " 'JJR': 853,\n",
       " 'WDT': 955,\n",
       " 'WRB': 478,\n",
       " 'RBS': 191,\n",
       " 'PDT': 55,\n",
       " 'RP': 83,\n",
       " ':': 1047,\n",
       " 'FW': 38,\n",
       " 'WP$': 35,\n",
       " 'SYM': 6,\n",
       " 'UH': 15}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cnt = count_pos(train_corpus)\n",
    "pos_cnt\n",
    "\"\"\" Output:\n",
    "{'NN': 30147,\n",
    " 'IN': 22764,\n",
    " 'NNS': 13619,\n",
    " 'NNP': 19884,\n",
    " ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the chunk distribution for each part of speech. You will use the training file to derive the distribution and you will store the results in a dictionary. Below, you have an excerpt of the expected results:\n",
    "```\n",
    "{'JJR':\n",
    "{'I-ADVP': 17, 'I-ADJP': 45, 'I-NP': 204, 'B-ADVP': 63,\n",
    "'B-PP': 2, 'B-ADJP': 111, 'B-NP': 382, 'B-VP': 2,\n",
    "'I-VP': 11, 'O': 16},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def chunk_distribution(corpus):\n",
    "    chunk_dist = defaultdict(dict)\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            _, pos, chunk = row.values()\n",
    "            if chunk in chunk_dist[pos]:\n",
    "                chunk_dist[pos][chunk] += 1\n",
    "            else:\n",
    "                chunk_dist[pos][chunk] = 1\n",
    "    return chunk_dist\n",
    "\n",
    "chunk_dist = chunk_distribution(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-NP': 5160,\n",
       " 'I-NP': 24456,\n",
       " 'B-VP': 257,\n",
       " 'B-ADJP': 44,\n",
       " 'B-ADVP': 38,\n",
       " 'O': 37,\n",
       " 'B-PP': 15,\n",
       " 'I-ADVP': 11,\n",
       " 'I-ADJP': 41,\n",
       " 'I-VP': 77,\n",
       " 'B-INTJ': 1,\n",
       " 'B-LST': 2,\n",
       " 'B-UCP': 2,\n",
       " 'I-UCP': 2,\n",
       " 'B-PRT': 2,\n",
       " 'I-INTJ': 2}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dist['NN']\n",
    "\"\"\"\n",
    "{'B-NP': 5160,\n",
    " 'I-NP': 24456,\n",
    " 'B-VP': 257,\n",
    " 'B-ADJP': 44,\n",
    " 'B-ADVP': 38,\n",
    " '...\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the POS-chunk associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each part of speech, select the best association. In the example above, you will have (NN, I-NP) as it is the most frequent. You will store the results in a dictionary that you will call `pos_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all dicts in chunk dist\n",
    "\n",
    "pos_chunk = {}\n",
    "\n",
    "for pos in chunk_dist.keys():\n",
    "    _dict = chunk_dist[pos]\n",
    "    max_key = max(_dict, key=_dict.get)\n",
    "    pos_chunk[pos] = max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-NP'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_chunk['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the resulting associations, apply your chunker to the test file. You will write a `predict(model, corpus)` function, where `model` will be your associations and `corpus`, the test corpus. You will format the test corpus as a dictionary, where you will add a `pchunk` key for each row with a value that will correspond to the predicted chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, corpus):\n",
    "    predicted_corpus = corpus.copy()\n",
    "    for sent_idx, sentence in enumerate(corpus):\n",
    "        for row_idx, row in enumerate(sentence):\n",
    "            _, pos, _ = row.values()\n",
    "            predicted_corpus[sent_idx][row_idx]['pchunk'] = model[pos]\n",
    "    return predicted_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:1]\n",
    "\"\"\"\n",
    "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
    "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
    "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
    "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the groups. You should have added a `pchunk` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_corpus = predict(pos_chunk, test_corpus)\n",
    "predicted_test_corpus[:1]\n",
    "\"\"\"\n",
    "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
    "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
    "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
    "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the baseline with the tag accuracy: the percentage of words that receive the correct tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the predicted chunk accuracy\n",
    "    :param predicted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_cnt = 0\n",
    "    correct = 0\n",
    "    for sentence in predicted:\n",
    "        for row in sentence:\n",
    "            word_cnt += 1\n",
    "            if row['chunk'] == row['pchunk']:\n",
    "                correct += 1\n",
    "    return correct / word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = eval(predicted_test_corpus)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoNLL evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very misleading as it is biased by the most frequent tags. It is not a good way to evaluate chunking. Instead, CoNLL computes the F1 score of all the chunks with a specific evaluation script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the CoNLL evaluation script, you will store your results in an output file that has four columns. The three first columns will be the input columns from the test file: \n",
    "* word, \n",
    "* part of speech, and \n",
    "* gold-standard chunk. \n",
    "\n",
    "You will append the predicted chunk as the 4th column. Your output file should look like the excerpt below:\n",
    "```\n",
    "Rockwell NNP B-NP I-NP\n",
    "International NNP I-NP I-NP\n",
    "Corp. NNP I-NP I-NP\n",
    "'s POS B-NP B-NP\n",
    "Tulsa NNP I-NP I-NP\n",
    "unit NN I-NP I-NP\n",
    "said VBD B-VP B-VP\n",
    "it PRP B-NP B-NP\n",
    "```\n",
    "The separator is the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a `save_results(output_dict, keys, output_file)` function, where the keys will be `['form', 'pos', 'chunk', 'pchunk']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dict, keys, output_file):\n",
    "    f_out = open(output_file, 'w')\n",
    "    # We write the word (form), part of speech (pos),\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\n",
    "    for sentence in output_dict:\n",
    "        for row in sentence:\n",
    "            for key in keys:\n",
    "                f_out.write(row[key] + ' ')\n",
    "            f_out.write('\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(predicted_test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 evaluation script will use these two last columns, chunk and predicted chunk, to compute the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: A first ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply and explore a machine-learning program.\n",
    "\n",
    "The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a window of five words around the chunk tag to identify, $c_i$. They built a feature vector consisting of:\n",
    "1. The values of the five words in this window: $w_{i-2}, w_{i-1}, w_{i}, w_{i+1}, w_{i+2}$\n",
    "2. The values of the five parts of speech in this window: $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}, t_{i+2}$\n",
    "3. The values of the two previous chunk tags in the first part of the window: $c_{i-2}, c_{i-1}$\n",
    "\n",
    "The two last parameters (3.) are said to be dynamic because the program computes them at run-time. Read [Kudoh and Matsumoto's paper](http://www.clips.uantwerpen.be/conll2000/pdf/14244kud.pdf) and the [Yamcha](http://www.chasen.org/~taku/software/yamcha/) software site.\n",
    "\n",
    "You will start with a given code that uses the two first sets of features (1. and 2.) and add yourself the last one (3.) to improve the performance of your chunker. Kudoh and Matsumoto trained a classifier based on support vector machines. You will use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first function to extract features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_static(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_static(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_static(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the window and the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the corpus and format it as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN'}]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict, y = extract_features_static(train_corpus, w_size, feature_names)\n",
    "X_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'B-PP']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the sentences and create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\n",
    "X_test_dict[:2]\n",
    "\"\"\"\n",
    "[{'word_n2': 'bos',\n",
    "  'word_n1': 'bos',\n",
    "  'word': 'rockwell',\n",
    "  'word_p1': 'international',\n",
    "  'word_p2': 'corp.',\n",
    "  'pos_n2': 'BOS',\n",
    "  'pos_n1': 'BOS',\n",
    "  'pos': 'NNP',\n",
    "  'pos_p1': 'NNP',\n",
    "  'pos_p2': 'NNP'},\n",
    "  ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-NP', 'I-NP'], dtype='<U7')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = classifier.predict(X_test)\n",
    "y_test_predicted[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the predicted chunks to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index sould be equal to the length of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question on the ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the feature vector that corresponds to the <tt>ml_chunker.py</tt> program? Is it the same Kudoh\n",
    "    and Matsumoto used in their experiment?\n",
    "2. What is the performance of the chunker?\n",
    "3. Remove the lexical features (the words) from the feature vector and measure the performance. You should\n",
    "    observe a decrease.\n",
    "4. What is the classifier used in the program? \n",
    "5. As an optional task, you may try two other classifiers from sklearn and measure their performance: decision trees, perceptron, support vector machines, etc. Be aware that support vector machines take a long time to train: up to one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: Adding all the features from Kudoh and Matsumoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the feature vector used in the previous section with the two dynamic features, $c_{i-2}, c_{i-1}$, and train a new model. You will need to write a new `extract_features_sent_dyn` and `predict` functions. \n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with logistic regression and the default parameters from sklearn, i.e. `linear_model.LogisticRegression()`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A frequent mistake in the labs** is to use the gold-standard chunks from the test set. Be aware that  when you predict the test set, you do not know the dynamic features in advance and you must  not use the ones from the test file. You will use the two previous chunk tags that you have predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to reach a global F1 score of 92 to pass this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_dyn(sentence, w_size, feature_names):\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        for j in range(w_size):\n",
    "            x.append(padded_sentence[i + j]['chunk'])\n",
    "        \n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dyn(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_dyn(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 2\n",
    "feature_names_dyn = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                     'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2', \n",
    "                     'chunk_n2', 'chunk_n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict, y = extract_features_dyn(train_corpus, w_size, feature_names_dyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now vectorize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=True)\n",
    "X_train = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "classifier2 = linear_model.LogisticRegression()\n",
    "model2 = classifier2.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will finally predict the test set. We load the corpus again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the static features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static([test_corpus[0]], w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{X}\\_{\\textrm{dict}}$ is incomplete. For the prediction, we need to reinject dynamically the two previously predicted tags to have the full feature vector. Write this code here. \n",
    "\n",
    "This part is probably the most difficult of the lab. You may want to write it first for one sentence, and then for the test corpus. The prediction will take a longer time and you may want to include a progress bar with this snippet: \n",
    "```\n",
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2012 [00:03<06:13,  5.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-416daf4156d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mX_test_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chunk_n2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_n2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_test_predicted_dyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 290\u001b[0;31m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[0;32m--> 490\u001b[0;31m            other.ravel(), result.ravel())\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "    chunk_n1, chunk_n2 = 'BOS', 'BOS'\n",
    "    X_test_dict, y_test = extract_features_static([test_sentence], w_size, feature_names)\n",
    "    for i in range(len(X_test_dict)):\n",
    "        X_test_dict[i]['chunk_n1'] = chunk_n1\n",
    "        X_test_dict[i]['chunk_n2'] = chunk_n2\n",
    "        X_test = vec.transform(X_test_dict[i])\n",
    "        y_pred = classifier2.predict(X_test)\n",
    "        \n",
    "        y_test_predicted_dyn.extend(y_pred)\n",
    "        chunk_n2 = chunk_n1\n",
    "        chunk_n1 = y_pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'I-NP']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_dyn[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted_dyn[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9265266775640221"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "improved_ml_score = res['overall']['chunks']['evals']['f1']\n",
    "improved_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now collect all the named entities from the training set, defined as NP chunks and starting with a `NNP` (proper noun) or a `NNPS` (proper noun, plural) tag. As an example, in the first sentence of `train_corpus`, you will extract `('September', )` and `('July', 'and', 'August')`. You will set all the tuples in a set that you will call `ne_set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a two-pass procedure. For each sentence of the corpus:\n",
    "1. In the first pass, you will collect the start indices of the noun groups which are also proper nouns. For the first sentence, it will result in the list `[16, 30]`;\n",
    "2. In the second pass, you will collect the segments, starting at each index. For the first sentence, it will result in the tuples `('September',)`and `('July', 'and', 'August')`\n",
    "\n",
    "Should you have a better solution, please use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_set = set()\n",
    "for sentence in train_corpus:\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        idxs = []\n",
    "        # If word at index i in sentence is B-NP:\n",
    "        if sentence[i]['pos'] in ['NNP', 'NNPS'] and sentence[i]['chunk'] == 'B-NP':\n",
    "            idxs.append(i)\n",
    "            i += 1\n",
    "            while i < len(sentence) and sentence[i]['chunk'] == 'I-NP':\n",
    "                idxs.append(i)\n",
    "                i += 1\n",
    "            # Add tuple of the forms at the extracted word indexes to ne_set\n",
    "            _set = []\n",
    "            for idx in idxs:\n",
    "                _set.append(sentence[idx]['form'])\n",
    "            ne_set.add(tuple(_set))\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lotus', 'Development'),\n",
       " ('Chandler',),\n",
       " ('Mr.', 'Lowell'),\n",
       " ('Economic', 'Advisers'),\n",
       " ('R.I',),\n",
       " ('Moody', \"'s\"),\n",
       " ('Minnesota', 'Mining', '&', 'Manufacturing', 'Co.'),\n",
       " ('RJR', 'Holdings', 'Capital', 'Corp.'),\n",
       " ('Lewis', 'Galoob', 'Toys'),\n",
       " ('September', '1983')]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ne_set)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a small set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the subsequent experiments faster, you will limit the dataset to the entities starting with letter _K_. I chose this letter, because it corresponded to one of the smallest sets. You will call the resulting set: `ne_small_set`. Feel free to use the full set after you have completed this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_small_set = set(entity for entity in ne_set if entity[0][0].lower() == 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a simple method to find the named entities from the previous exercise in Wikipedia and Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to lookup entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the function below and try to understand what it means. You will describe it in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_lookup(ner, base_url='https://en.wikipedia.org/wiki/'):\n",
    "    try:\n",
    "        url_en = base_url + ' '.join(ner)\n",
    "        html_doc = requests.get(url_en).text\n",
    "        parse_tree = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        entity_id = parse_tree.find(\"a\", {\"accesskey\": \"g\"})['href']\n",
    "        head_id, entity_id = os.path.split(entity_id)\n",
    "        return entity_id\n",
    "    except:\n",
    "        pass\n",
    "        # print('Not found in: ', base_url)\n",
    "    entity_id = 'UNK'\n",
    "    return entity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to run the lookup and keep the resolved entities (only the resolved entities). You will call it `ne_ids_en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_ids_en = []\n",
    "for entity in ne_small_set:\n",
    "    entity_id = wikipedia_lookup(entity)\n",
    "    if entity_id != 'UNK':\n",
    "        ne_ids_en.append((entity, entity_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, entities need a confirmation. You will apply the resolution with the Swedish wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_ids_sv = []\n",
    "for entity in ne_small_set:\n",
    "    entity_id = wikipedia_lookup(entity, base_url='https://sv.wikipedia.org/wiki/')\n",
    "    if entity_id != 'UNK':\n",
    "        ne_ids_sv.append((entity, entity_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the intersection of the two sets. You will assign it to a list that you will sort and that you will call: `confirmed_ne_en_sv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_ne_en_sv = list(set(ne_ids_en).intersection(set(ne_ids_sv)))\n",
    "confirmed_ne_en_sv = sorted(confirmed_ne_en_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first items in your list should look like:\n",
    "```\n",
    "[(('KIM',), 'Q224736'),\n",
    " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
